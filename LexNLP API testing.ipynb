{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lexnlp.extract.en.amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A late charge of $50 will be incurred if rent is not paid when due. Sublandlord will have all rights of the Master Landlord with respect to eviction if late rent is not paid.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50.0]\n",
      "15\n",
      "2035000000.0000002\n"
     ]
    }
   ],
   "source": [
    "print(list(lexnlp.extract.en.amounts.get_amounts(text)))\n",
    "print(lexnlp.extract.en.amounts.text2num(\"ten one one three\")) # All text are added together and return the sum\n",
    "print(lexnlp.extract.en.amounts.text2num(\"2.035 billion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('if', 'A late charge of $50 will be incurred', ''), ('when', 'rent is not paid', ''), ('if', 'Sublandlord will have all rights of the Master Landlord with respect to eviction', '')]\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.conditions\n",
    "\n",
    "print(list(lexnlp.extract.en.conditions.get_conditions(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    " import lexnlp.extract.en.constraints\n",
    "\n",
    " print(list(lexnlp.extract.en.constraints.get_constraints(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 'F.2d', 'Federal Reporter', 234, None, None, 1999)]\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.citations\n",
    "text = \"Based on the precedent set in Doe v.  Acme, 100 F.2d 234 (1999)\"\n",
    "print(list(lexnlp.extract.en.citations.get_citations(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('unless', 'This will occur', '')]\n"
     ]
    }
   ],
   "source": [
    "#Extracting conditional statements\n",
    "import lexnlp.extract.en.conditions\n",
    "text = \"This will occur unless something else happens.\"\n",
    "print(list(lexnlp.extract.en.conditions.get_conditions(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = \"This will occur predicated upon something else.\"\n",
    "print(list(lexnlp.extract.en.conditions.get_conditions(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('at most', 'this will happen', '')]\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.constraints\n",
    "text = \"This will happen at most hundred times.\"\n",
    "print(list(lexnlp.extract.en.constraints.get_constraints(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('no less than', 'the rate shall be', '')]\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.conditions\n",
    "text2 = \"The rate shall be no less than 50 bps.\"\n",
    "print(list(lexnlp.extract.en.constraints.get_constraints(text2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Copyright', '1993-1996', 'Hughes Information Systems Company')]\n"
     ]
    }
   ],
   "source": [
    " import lexnlp.extract.en.copyright\n",
    "text = \"(C) Copyright 1993-1996 Hughes Information Systems Company\"\n",
    "print(list(lexnlp.extract.en.copyright.get_copyright(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('©', '2017', 'SIGN LLC')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Test copyrigh symbol © 2017, SIGN LLC\"\n",
    "print(list(lexnlp.extract.en.conditions.get_conditions(text)))\n",
    "print(list(lexnlp.extract.en.copyright.get_copyright(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-8476a46d00d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlexnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcourts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The case will be heard in E.D. Va. next month\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m court_config_data = [entity_config(0, \"Eastern District of Virginia\", 0, [\"E.D. Va.\"]),\n\u001b[0m\u001b[1;32m      4\u001b[0m     entity_config(1, \"Western District of Virginia\", 0, [\"W.D. Va.\"])]\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malias\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcourts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_courts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcourt_config_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_config' is not defined"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.courts\n",
    "text = \"The case will be heard in E.D. Va. next month\"\n",
    "court_config_data = [entity_config(0, \"Eastern District of Virginia\", 0, [\"E.D. Va.\"]),\n",
    "    entity_config(1, \"Western District of Virginia\", 0, [\"W.D. Va.\"])]\n",
    "for entity, alias in lexnlp.extract.en.courts.get_courts(text, court_config_data):\n",
    "    print(\"entity=\", entity)\n",
    "    print(\"alias=\", alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Client']\n"
     ]
    }
   ],
   "source": [
    " #Extracting constraints\n",
    "    import lexnlp.extract.en.definitions\n",
    "text = 'and Acme, LLC (\"Client\")'\n",
    "print(list(lexnlp.extract.en.definitions.get_definitions(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('month', 9.0, 270.0)]\n",
      "[('second', 12, 0.0001388888888888889)]\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.durations\n",
    "text = \"This Agreement shall terminate in nine (9) months.\"\n",
    "print(list(lexnlp.extract.en.durations.get_durations(text)))\n",
    "text2 = \"The period shall not exceed a dozen seconds.\"\n",
    "print(list(lexnlp.extract.en.durations.get_durations(text2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_geoentities() missing 1 required positional argument: 'geo_config_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-f495022e34f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlexnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I was born in Vannes and I live in Barcelona\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_geoentities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: get_geoentities() missing 1 required positional argument: 'geo_config_list'"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.geoentities\n",
    "text = \"I was born in Vannes and I live in Barcelona\"\n",
    "print(list(lexnlp.extract.en.geoentities.get_geoentities(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(500.0, 'INR')]\n",
      "[(10000000.0, 'USD')]\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.money\n",
    "text1 = \"The price will be INR 500\"\n",
    "text2 = \"10 million dollar\"\n",
    "print(list(lexnlp.extract.en.money.get_money(text1)))\n",
    "print(list(lexnlp.extract.en.money.get_money(text2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://elevateservices.eightfold.ai/careers']\n"
     ]
    }
   ],
   "source": [
    "import lexnlp.extract.en.urls\n",
    "text = \"Here you can access the elevate carrier page https://elevateservices.eightfold.ai/careers\"\n",
    "print(list(lexnlp.extract.en.urls.get_urls(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/prashant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "['dummy']\n",
      "['simply']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import lexnlp.nlp.en.tokens\n",
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"\n",
    "print(list(lexnlp.nlp.en.tokens.get_adjectives(text)))\n",
    "print(list(lexnlp.nlp.en.tokens.get_adverbs(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object get_tokens at 0x7f44727e81a8>\n",
      "['lorem', 'ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'industry', '.']\n",
      "['lorem', 'ipsum', 'simply', 'dummy', 'text', 'printing', 'typesetting', 'industry', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lexnlp.nlp.en.tokens.get_tokens(text))\n",
    "print(lexnlp.nlp.en.tokens.get_token_list(text, lowercase=True))\n",
    "print(lexnlp.nlp.en.tokens.get_token_list(text, lowercase=True, stopword=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/prashant/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "<generator object get_stems at 0x7f446fde9eb8>\n",
      "['lorem', 'ipsum', 'industri', \"'s\", 'standard', 'dummi', 'text', 'ever', 'sinc', '1500s']\n",
      "['lorem', 'ips', 'has', 'been', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ev', 'sint', 'the', '1500s']\n",
      "['Lorem', 'Ipsum', 'have', 'be', 'the', 'industry', \"'s\", 'standard', 'dummy', 'text', 'ever', 'since', 'the', '1500s']\n",
      "['lorem', 'ipsum', 'simply', 'dummy', 'text', 'printing', 'typesetting', 'industry', '.']\n"
     ]
    }
   ],
   "source": [
    "#Stemming and lemmatizing text\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "textforstem = \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s\"\n",
    "print(lexnlp.nlp.en.tokens.get_stems(textforstem))\n",
    "print(lexnlp.nlp.en.tokens.get_stem_list(textforstem, stopword=True))\n",
    "print(lexnlp.nlp.en.tokens.get_stem_list(textforstem, stemmer=nltk.stem.lancaster.LancasterStemmer()))\n",
    "print(lexnlp.nlp.en.tokens.get_lemma_list(textforstem))\n",
    "print(lexnlp.nlp.en.tokens.get_lemma_list(text, stopword=True, lowercase=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['centuries', 'leap', 'typesetting']\n",
      "['has', 'survived', 'remaining']\n",
      "['have', 'survive', 'remain']\n",
      "['electronic', 'unchanged']\n",
      "['not', 'only', 'also', 'essentially']\n"
     ]
    }
   ],
   "source": [
    "#parts-of-speech\n",
    "text = \"It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged\"\n",
    "print(list(lexnlp.nlp.en.tokens.get_nouns(text)))\n",
    "print(list(lexnlp.nlp.en.tokens.get_verbs(text)))\n",
    "print(list(lexnlp.nlp.en.tokens.get_verbs(text, lemmatize=True)))\n",
    "print(list(lexnlp.nlp.en.tokens.get_adjectives(text)))\n",
    "print(list(lexnlp.nlp.en.tokens.get_adverbs(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
